{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d2d05e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97142bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbe358",
   "metadata": {},
   "source": [
    "# Connect to PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb2abe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/credentials.json\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "username = creds[\"POSTGRES_USERNAME\"]\n",
    "password = creds[\"POSTGRES_PASSWORD\"]\n",
    "host = creds[\"POSTGRES_HOST\"]\n",
    "port = creds[\"POSTGRES_PORT\"]\n",
    "dbname = creds[\"POSTGRES_DBNAME\"]\n",
    "\n",
    "DB_URL = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80119cd",
   "metadata": {},
   "source": [
    "# View Database Schemas and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea0d606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas:\n",
      " - information_schema\n",
      " - pg_catalog\n",
      " - public\n",
      "\n",
      "Tables:\n",
      " - information_schema.sql_features\n",
      " - information_schema.sql_implementation_info\n",
      " - information_schema.sql_parts\n",
      " - information_schema.sql_sizing\n",
      " - pg_catalog.pg_aggregate\n",
      " - pg_catalog.pg_am\n",
      " - pg_catalog.pg_amop\n",
      " - pg_catalog.pg_amproc\n",
      " - pg_catalog.pg_attrdef\n",
      " - pg_catalog.pg_attribute\n",
      " - pg_catalog.pg_auth_members\n",
      " - pg_catalog.pg_cast\n",
      " - pg_catalog.pg_class\n",
      " - pg_catalog.pg_collation\n",
      " - pg_catalog.pg_constraint\n",
      " - pg_catalog.pg_conversion\n",
      " - pg_catalog.pg_database\n",
      " - pg_catalog.pg_db_role_setting\n",
      " - pg_catalog.pg_default_acl\n",
      " - pg_catalog.pg_depend\n",
      " - pg_catalog.pg_description\n",
      " - pg_catalog.pg_enum\n",
      " - pg_catalog.pg_event_trigger\n",
      " - pg_catalog.pg_extension\n",
      " - pg_catalog.pg_foreign_data_wrapper\n",
      " - pg_catalog.pg_foreign_server\n",
      " - pg_catalog.pg_foreign_table\n",
      " - pg_catalog.pg_index\n",
      " - pg_catalog.pg_inherits\n",
      " - pg_catalog.pg_init_privs\n",
      " - pg_catalog.pg_language\n",
      " - pg_catalog.pg_largeobject_metadata\n",
      " - pg_catalog.pg_namespace\n",
      " - pg_catalog.pg_opclass\n",
      " - pg_catalog.pg_operator\n",
      " - pg_catalog.pg_opfamily\n",
      " - pg_catalog.pg_parameter_acl\n",
      " - pg_catalog.pg_partitioned_table\n",
      " - pg_catalog.pg_policy\n",
      " - pg_catalog.pg_proc\n",
      " - pg_catalog.pg_publication\n",
      " - pg_catalog.pg_publication_namespace\n",
      " - pg_catalog.pg_publication_rel\n",
      " - pg_catalog.pg_range\n",
      " - pg_catalog.pg_replication_origin\n",
      " - pg_catalog.pg_rewrite\n",
      " - pg_catalog.pg_seclabel\n",
      " - pg_catalog.pg_sequence\n",
      " - pg_catalog.pg_shdepend\n",
      " - pg_catalog.pg_shdescription\n",
      " - pg_catalog.pg_shseclabel\n",
      " - pg_catalog.pg_statistic_ext\n",
      " - pg_catalog.pg_subscription\n",
      " - pg_catalog.pg_subscription_rel\n",
      " - pg_catalog.pg_tablespace\n",
      " - pg_catalog.pg_transform\n",
      " - pg_catalog.pg_trigger\n",
      " - pg_catalog.pg_ts_config\n",
      " - pg_catalog.pg_ts_config_map\n",
      " - pg_catalog.pg_ts_dict\n",
      " - pg_catalog.pg_ts_parser\n",
      " - pg_catalog.pg_ts_template\n",
      " - pg_catalog.pg_type\n"
     ]
    }
   ],
   "source": [
    "# List all schemas and tables\n",
    "with engine.connect() as conn:\n",
    "    # List all schemas\n",
    "    schemas = conn.execute(text(\"\"\"\n",
    "        SELECT schema_name\n",
    "        FROM information_schema.schemata\n",
    "        ORDER BY schema_name;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "    # List all tables across all schemas\n",
    "    tables = conn.execute(text(\"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "# Display\n",
    "print(\"Schemas:\")\n",
    "for schema in schemas:\n",
    "    print(f\" - {schema[0]}\")\n",
    "\n",
    "print(\"\\nTables:\")\n",
    "for schema, table in tables:\n",
    "    print(f\" - {schema}.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc6f1c",
   "metadata": {},
   "source": [
    "# Create New Schema and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dafc167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or verified schema: quant\n",
      "Executed schema.sql and created all tables.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../config/credentials.json\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "schema = \"quant\"\n",
    "\n",
    "conn_str = f\"host={host} port={port} dbname={dbname} user={username} password={password}\"\n",
    "\n",
    "# Load the schema.sql file\n",
    "with open(\"../db/schema.sql\", \"r\") as f:\n",
    "    sql_script = f.read()\n",
    "\n",
    "# Inject schema prefix\n",
    "sql_script = sql_script.replace(\"CREATE TABLE \", f\"CREATE TABLE {schema}.\")\n",
    "\n",
    "# === Connect and execute ===\n",
    "with psycopg2.connect(conn_str) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Create schema first\n",
    "        cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "        print(f\"Created or verified schema: {schema}\")\n",
    "\n",
    "        # Execute all table creation statements\n",
    "        cur.execute(sql_script)\n",
    "        print(\"Executed schema.sql and created all tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51125d34",
   "metadata": {},
   "source": [
    "# View Database Schemas and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "787ea589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas:\n",
      " - information_schema\n",
      " - pg_catalog\n",
      " - public\n",
      " - quant\n",
      "\n",
      "Tables:\n",
      " - information_schema.sql_features\n",
      " - information_schema.sql_implementation_info\n",
      " - information_schema.sql_parts\n",
      " - information_schema.sql_sizing\n",
      " - pg_catalog.pg_aggregate\n",
      " - pg_catalog.pg_am\n",
      " - pg_catalog.pg_amop\n",
      " - pg_catalog.pg_amproc\n",
      " - pg_catalog.pg_attrdef\n",
      " - pg_catalog.pg_attribute\n",
      " - pg_catalog.pg_auth_members\n",
      " - pg_catalog.pg_cast\n",
      " - pg_catalog.pg_class\n",
      " - pg_catalog.pg_collation\n",
      " - pg_catalog.pg_constraint\n",
      " - pg_catalog.pg_conversion\n",
      " - pg_catalog.pg_database\n",
      " - pg_catalog.pg_db_role_setting\n",
      " - pg_catalog.pg_default_acl\n",
      " - pg_catalog.pg_depend\n",
      " - pg_catalog.pg_description\n",
      " - pg_catalog.pg_enum\n",
      " - pg_catalog.pg_event_trigger\n",
      " - pg_catalog.pg_extension\n",
      " - pg_catalog.pg_foreign_data_wrapper\n",
      " - pg_catalog.pg_foreign_server\n",
      " - pg_catalog.pg_foreign_table\n",
      " - pg_catalog.pg_index\n",
      " - pg_catalog.pg_inherits\n",
      " - pg_catalog.pg_init_privs\n",
      " - pg_catalog.pg_language\n",
      " - pg_catalog.pg_largeobject_metadata\n",
      " - pg_catalog.pg_namespace\n",
      " - pg_catalog.pg_opclass\n",
      " - pg_catalog.pg_operator\n",
      " - pg_catalog.pg_opfamily\n",
      " - pg_catalog.pg_parameter_acl\n",
      " - pg_catalog.pg_partitioned_table\n",
      " - pg_catalog.pg_policy\n",
      " - pg_catalog.pg_proc\n",
      " - pg_catalog.pg_publication\n",
      " - pg_catalog.pg_publication_namespace\n",
      " - pg_catalog.pg_publication_rel\n",
      " - pg_catalog.pg_range\n",
      " - pg_catalog.pg_replication_origin\n",
      " - pg_catalog.pg_rewrite\n",
      " - pg_catalog.pg_seclabel\n",
      " - pg_catalog.pg_sequence\n",
      " - pg_catalog.pg_shdepend\n",
      " - pg_catalog.pg_shdescription\n",
      " - pg_catalog.pg_shseclabel\n",
      " - pg_catalog.pg_statistic_ext\n",
      " - pg_catalog.pg_subscription\n",
      " - pg_catalog.pg_subscription_rel\n",
      " - pg_catalog.pg_tablespace\n",
      " - pg_catalog.pg_transform\n",
      " - pg_catalog.pg_trigger\n",
      " - pg_catalog.pg_ts_config\n",
      " - pg_catalog.pg_ts_config_map\n",
      " - pg_catalog.pg_ts_dict\n",
      " - pg_catalog.pg_ts_parser\n",
      " - pg_catalog.pg_ts_template\n",
      " - pg_catalog.pg_type\n",
      " - quant.macro_indicators\n",
      " - quant.ohlcv_tiingo\n",
      " - quant.ohlcv_yfinance\n",
      " - quant.sector_etf_prices\n",
      " - quant.ticker_metadata\n"
     ]
    }
   ],
   "source": [
    "# List all schemas and tables\n",
    "with engine.connect() as conn:\n",
    "    # List all schemas\n",
    "    schemas = conn.execute(text(\"\"\"\n",
    "        SELECT schema_name\n",
    "        FROM information_schema.schemata\n",
    "        ORDER BY schema_name;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "    # List all tables across all schemas\n",
    "    tables = conn.execute(text(\"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "# Display\n",
    "print(\"Schemas:\")\n",
    "for schema in schemas:\n",
    "    print(f\" - {schema[0]}\")\n",
    "\n",
    "print(\"\\nTables:\")\n",
    "for schema, table in tables:\n",
    "    print(f\" - {schema}.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cfc3ed",
   "metadata": {},
   "source": [
    "# Upload Ticker Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5d7ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in CSV: 647\n",
      "Data uploaded to quant.ticker_metadata\n",
      "Rows in quant.ticker_metadata: 647\n",
      "Sample data from DB:\n",
      "('A', '2017-01-01', '2025-06-22', 'XLV')\n",
      "('AAL', '2017-01-01', '2024-09-22', 'XLI')\n",
      "('AAP', '2017-01-01', '2023-08-24', 'XLY')\n",
      "('AAPL', '2017-01-01', '2025-06-22', 'XLK')\n",
      "('ABBV', '2017-01-01', '2025-06-22', 'XLV')\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(f\"postgresql://{username}:{password}@{host}:{port}/{dbname}\")\n",
    "\n",
    "# Load CSV\n",
    "csv_path = \"../data/final_tickers_date_sectors.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Rows in CSV: {len(df)}\")\n",
    "\n",
    "# Upload to quant.ticker_metadata\n",
    "df.to_sql(\"ticker_metadata\", engine, schema=schema, if_exists=\"replace\", index=False, method=\"multi\")\n",
    "print(\"Data uploaded to quant.ticker_metadata\")\n",
    "\n",
    "# Query the row count to verify\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM quant.ticker_metadata\"))\n",
    "    count_in_db = result.scalar()\n",
    "\n",
    "print(f\"Rows in quant.ticker_metadata: {count_in_db}\")\n",
    "\n",
    "# Preview the first few rows\n",
    "with engine.connect() as conn:\n",
    "    preview = conn.execute(text(\"SELECT * FROM quant.ticker_metadata LIMIT 5\")).fetchall()\n",
    "\n",
    "print(\"Sample data from DB:\")\n",
    "for row in preview:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad8655",
   "metadata": {},
   "source": [
    "# Upload Macro Indicator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca2a3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 16658 total rows from 7 files to quant.macro_indicators\n",
      "Total rows in quant.macro_indicators: 16658\n",
      "Sample data:\n",
      "('5Y', datetime.date(2016, 1, 4), 1.7350000143051147, 1.7350000143051147, 1.6979999542236328, 1.7200000286102295, 0)\n",
      "('DXY', datetime.date(2016, 1, 4), 98.87000274658205, 99.18000030517578, 98.0500030517578, 98.69000244140624, 0)\n",
      "('10Y', datetime.date(2016, 1, 4), 2.244999885559082, 2.244999885559082, 2.200000047683716, 2.2300000190734863, 0)\n",
      "('3M', datetime.date(2016, 1, 4), 0.1550000011920929, 0.1700000017881393, 0.152999997138977, 0.1650000065565109, 0)\n",
      "('GOLD', datetime.date(2016, 1, 4), 1075.0999755859375, 1082.5, 1063.199951171875, 1063.4000244140625, 143)\n"
     ]
    }
   ],
   "source": [
    "macro_folder = \"../data/yfinance/macro\"\n",
    "table_name = \"macro_indicators\"\n",
    "schema_name = \"quant\"\n",
    "\n",
    "# Count and upload\n",
    "all_rows = 0\n",
    "all_files = [f for f in os.listdir(macro_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(macro_folder, file)\n",
    "\n",
    "    # Read CSV, skipping first 2 rows and manually naming columns\n",
    "    df_raw = pd.read_csv(file_path, skiprows=2, names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"])\n",
    "    \n",
    "    # Add Ticker column from filename\n",
    "    ticker = file.replace(\".csv\", \"\")\n",
    "    df_raw[\"Ticker\"] = ticker\n",
    "\n",
    "     # Convert to lowercase column names for PostgreSQL compatibility\n",
    "    df_raw.columns = [col.lower() for col in df_raw.columns]\n",
    "    df_raw = df_raw[[\"ticker\", \"date\", \"close\", \"high\", \"low\", \"open\", \"volume\"]]\n",
    "\n",
    "    # Convert date and clean\n",
    "    df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], format=\"%Y-%m-%d\", errors='coerce').dt.date\n",
    "    df_raw = df_raw.dropna(subset=[\"date\", \"close\"])\n",
    "    \n",
    "    all_rows += len(df_raw)\n",
    "\n",
    "    # Upload to PostgreSQL\n",
    "    df_raw.to_sql(table_name, con=engine, schema=schema_name, if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "print(f\"Uploaded {all_rows} total rows from {len(all_files)} files to {schema_name}.{table_name}\")\n",
    "\n",
    "# Final check\n",
    "with engine.connect() as conn:\n",
    "    count = conn.execute(text(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")).scalar()\n",
    "    print(f\"Total rows in {schema_name}.{table_name}: {count}\")\n",
    "    \n",
    "    sample = conn.execute(text(f\"\"\"\n",
    "        SELECT * FROM {schema_name}.{table_name}\n",
    "        ORDER BY Date ASC\n",
    "        LIMIT 5;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "for row in sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af428fea",
   "metadata": {},
   "source": [
    "# Upload Sector ETF OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34d4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 23408 total rows from 11 sector ETF files to quant.sector_etf_prices\n",
      "Total rows in quant.sector_etf_prices: 23408\n",
      "Sample data:\n",
      "('XLE', datetime.date(2017, 1, 3), 76.17, 76.81, 75.36, 76.11, 24623100, 53.2964646707, 53.7442753231, 52.7297043139, 53.2544824221, 24623100, 0.0, 1.0)\n",
      "('XLF', datetime.date(2017, 1, 3), 23.51, 23.67, 23.26, 23.61, 71259897, 20.082916608, 20.2195931992, 19.8693594344, 20.1683394775, 71259897, 0.0, 1.0)\n",
      "('XLB', datetime.date(2017, 1, 3), 49.99, 50.27, 49.63, 49.83, 7737466, 42.3781205136, 42.6154854615, 42.0729370092, 42.2424834006, 7737466, 0.0, 1.0)\n",
      "('XLC', datetime.date(2017, 1, 3), 76.959412, 77.458762, 75.927021, 76.44, 8408766, 28.468815, 28.770687, 28.104405, 28.394991, 14714247, 0.0, 1.0)\n",
      "('XLI', datetime.date(2017, 1, 3), 62.59, 63.06, 62.35, 62.68, 21592200, 54.1442427058, 54.5508219369, 53.9366277793, 54.2220983033, 21592200, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "etf_folder = \"../data/tiingo/ohlcv\"\n",
    "table_name = \"sector_etf_prices\"\n",
    "schema_name = \"quant\"\n",
    "\n",
    "# Define the 11 sector ETFs\n",
    "sector_etfs = {\n",
    "    \"XLB\", \"XLC\", \"XLE\", \"XLF\", \"XLI\", \"XLK\",\n",
    "    \"XLP\", \"XLRE\", \"XLU\", \"XLV\", \"XLY\"\n",
    "}\n",
    "\n",
    "# Upload Loop\n",
    "all_rows = 0\n",
    "all_files = [f for f in os.listdir(etf_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in all_files:\n",
    "    ticker = file.replace(\".csv\", \"\")\n",
    "    if ticker not in sector_etfs:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(etf_folder, file)\n",
    "\n",
    "    # Read CSV\n",
    "    df_raw = pd.read_csv(file_path)\n",
    "\n",
    "    # Parse and rename columns\n",
    "    df_raw.columns = [col.lower() for col in df_raw.columns]\n",
    "    df_raw[\"etf\"] = ticker  # Add ETF name column\n",
    "\n",
    "    # Convert date\n",
    "    df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors='coerce').dt.date\n",
    "    df_raw = df_raw.dropna(subset=[\"date\", \"close\"])\n",
    "\n",
    "    # Reorder to match schema\n",
    "    df_raw = df_raw[\n",
    "        [\"etf\", \"date\", \"close\", \"high\", \"low\", \"open\", \"volume\",\n",
    "         \"adjclose\", \"adjhigh\", \"adjlow\", \"adjopen\", \"adjvolume\",\n",
    "         \"divcash\", \"splitfactor\"]\n",
    "    ]\n",
    "\n",
    "    all_rows += len(df_raw)\n",
    "\n",
    "    # Upload\n",
    "    df_raw.to_sql(table_name, con=engine, schema=schema_name, if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "print(f\"Uploaded {all_rows} total rows from {len(sector_etfs)} sector ETF files to {schema_name}.{table_name}\")\n",
    "\n",
    "# Final DB Check\n",
    "with engine.connect() as conn:\n",
    "    count = conn.execute(text(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")).scalar()\n",
    "    print(f\"Total rows in {schema_name}.{table_name}: {count}\")\n",
    "\n",
    "    sample = conn.execute(text(f\"\"\"\n",
    "        SELECT * FROM {schema_name}.{table_name}\n",
    "        ORDER BY date ASC\n",
    "        LIMIT 5;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "for row in sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70552292",
   "metadata": {},
   "source": [
    "# Upload Tiingo Ticker OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0baeab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1256197 total rows from non-sector files to quant.ohlcv_tiingo\n",
      "Total rows in quant.ohlcv_tiingo: 1256197\n",
      "Sample data:\n",
      "('AAP', datetime.date(2017, 1, 3), 170.6, 171.36, 169.31, 170.78, 691526, 151.9407591699, 152.6176347676, 150.7918519053, 152.1010718114, 691526, 0.0, 1.0)\n",
      "('AAPL', datetime.date(2017, 1, 3), 116.15, 116.33, 114.76, 115.8, 28781865, 26.8269201165, 26.8684943362, 26.505874753, 26.7460813559, 115127460, 0.0, 1.0)\n",
      "('A', datetime.date(2017, 1, 3), 46.49, 46.75, 45.74, 45.93, 1739726, 43.6575100207, 43.9016690357, 42.9532051699, 43.1316290654, 1739726, 0.0, 1.0)\n",
      "('AAL', datetime.date(2017, 1, 3), 46.3, 47.34, 46.135, 47.28, 6737752, 44.7427003579, 45.7477199771, 44.5832501298, 45.689738076, 6737752, 0.0, 1.0)\n",
      "('ABBV', datetime.date(2017, 1, 3), 62.41, 63.03, 61.935, 62.92, 9328198, 43.4426100466, 43.8741822021, 43.1119700887, 43.7976129487, 9328198, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "folder = \"../data/tiingo/ohlcv\"\n",
    "table_name = \"ohlcv_tiingo\"\n",
    "schema_name = \"quant\"\n",
    "\n",
    "# Exclude these 11 sector ETFs\n",
    "excluded_etfs = {\n",
    "    \"XLB\", \"XLC\", \"XLE\", \"XLF\", \"XLI\", \"XLK\",\n",
    "    \"XLP\", \"XLRE\", \"XLU\", \"XLV\", \"XLY\"\n",
    "}\n",
    "\n",
    "# Load additional ignore list\n",
    "ignore_file = \"../data/tiingo/tiingo_ignore_tickers.txt\"\n",
    "with open(ignore_file, \"r\") as f:\n",
    "    ignored_tickers = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "\n",
    "# Combine both sets\n",
    "excluded_tickers = excluded_etfs.union(ignored_tickers)\n",
    "\n",
    "# Upload Loop\n",
    "all_rows = 0\n",
    "all_files = [f for f in os.listdir(etf_folder) if f.endswith(\".csv\")]\n",
    "skipped_empty = []\n",
    "\n",
    "for file in all_files:\n",
    "    ticker = file.replace(\".csv\", \"\")\n",
    "    if ticker in excluded_tickers:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(etf_folder, file)\n",
    "\n",
    "    # Skip empty files\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        skipped_empty.append(ticker)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        skipped_empty.append(ticker)\n",
    "        continue\n",
    "\n",
    "    # Parse and rename\n",
    "    df_raw.columns = [col.lower() for col in df_raw.columns]\n",
    "    df_raw[\"ticker\"] = ticker\n",
    "\n",
    "    # Convert and clean\n",
    "    df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors='coerce').dt.date\n",
    "    df_raw = df_raw.dropna(subset=[\"date\", \"close\"])\n",
    "\n",
    "    # Reorder to match schema\n",
    "    df_raw = df_raw[\n",
    "        [\"ticker\", \"date\", \"close\", \"high\", \"low\", \"open\", \"volume\",\n",
    "         \"adjclose\", \"adjhigh\", \"adjlow\", \"adjopen\", \"adjvolume\",\n",
    "         \"divcash\", \"splitfactor\"]\n",
    "    ]\n",
    "\n",
    "    all_rows += len(df_raw)\n",
    "\n",
    "    # Upload\n",
    "    df_raw.to_sql(table_name, con=engine, schema=schema_name, if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "print(f\"Uploaded {all_rows} total rows from non-sector files to {schema_name}.{table_name}\")\n",
    "if skipped_empty:\n",
    "    print(f\"Skipped {len(skipped_empty)} empty files: {skipped_empty}\")\n",
    "\n",
    "# Final DB Check\n",
    "with engine.connect() as conn:\n",
    "    count = conn.execute(text(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")).scalar()\n",
    "    print(f\"Total rows in {schema_name}.{table_name}: {count}\")\n",
    "\n",
    "    sample = conn.execute(text(f\"\"\"\n",
    "        SELECT * FROM {schema_name}.{table_name}\n",
    "        ORDER BY date ASC\n",
    "        LIMIT 5;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "for row in sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058869b7",
   "metadata": {},
   "source": [
    "# Upload Yfinance Ticker OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ed2511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 8512 total rows from 4 files to quant.ohlcv_yfinance\n",
      "Total rows in quant.ohlcv_yfinance: 8512\n",
      "Sample data:\n",
      "('TKO', datetime.date(2017, 1, 3), 18.239999771118164, 18.56999969482422, 18.0, 18.56999969482422, 606000, 16.363698959350586, 18.56999969482422, 18.0, 18.56999969482422, 606000, 0.0, 1.0)\n",
      "('VTRS', datetime.date(2017, 1, 3), 39.11000061035156, 39.27999877929688, 38.41999816894531, 38.47999954223633, 5031900, 32.707218170166016, 39.27999877929688, 38.41999816894531, 38.47999954223633, 5031900, 0.0, 1.0)\n",
      "('DOC', datetime.date(2017, 1, 3), 29.780000686645508, 29.920000076293945, 29.520000457763672, 29.8799991607666, 3083500, 19.266828536987305, 29.920000076293945, 29.520000457763672, 29.8799991607666, 3083500, 0.0, 1.0)\n",
      "('LIN', datetime.date(2017, 1, 3), 116.9000015258789, 118.22000122070312, 116.12000274658205, 117.97000122070312, 2364200, 101.37499237060548, 118.22000122070312, 116.12000274658205, 117.97000122070312, 2364200, 0.0, 1.0)\n",
      "('DOC', datetime.date(2017, 1, 4), 30.440000534057617, 30.56999969482422, 29.809999465942383, 29.88999938964844, 5343100, 19.69382667541504, 30.56999969482422, 29.809999465942383, 29.88999938964844, 5343100, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "folder = \"../data/yfinance/ohlcv\"\n",
    "table_name = \"ohlcv_yfinance\"\n",
    "schema_name = \"quant\"\n",
    "\n",
    "# Upload Loop\n",
    "all_rows = 0\n",
    "skipped_empty = []\n",
    "all_files = [f for f in os.listdir(folder) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in all_files:\n",
    "    ticker = file.replace(\".csv\", \"\")\n",
    "    file_path = os.path.join(folder, file)\n",
    "\n",
    "    # Skip empty or unreadable files\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        skipped_empty.append(ticker)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        skipped_empty.append(ticker)\n",
    "        continue\n",
    "\n",
    "    # Rename + lowercase for DB compatibility\n",
    "    df_raw.columns = [col.lower() for col in df_raw.columns]\n",
    "    df_raw[\"ticker\"] = ticker\n",
    "\n",
    "    # Convert date and clean\n",
    "    df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors='coerce').dt.date\n",
    "    df_raw = df_raw.dropna(subset=[\"date\", \"close\"])\n",
    "\n",
    "    # Reorder to match schema\n",
    "    df_raw = df_raw[\n",
    "        [\"ticker\", \"date\", \"close\", \"high\", \"low\", \"open\", \"volume\",\n",
    "         \"adjclose\", \"adjhigh\", \"adjlow\", \"adjopen\", \"adjvolume\",\n",
    "         \"divcash\", \"splitfactor\"]\n",
    "    ]\n",
    "\n",
    "    all_rows += len(df_raw)\n",
    "\n",
    "    # Upload\n",
    "    df_raw.to_sql(table_name, con=engine, schema=schema_name,\n",
    "                  if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "print(f\"Uploaded {all_rows} total rows from {len(all_files)} files to {schema_name}.{table_name}\")\n",
    "if skipped_empty:\n",
    "    print(f\"Skipped {len(skipped_empty)} empty files: {skipped_empty}\")\n",
    "\n",
    "# Final DB Check\n",
    "with engine.connect() as conn:\n",
    "    count = conn.execute(text(f\"SELECT COUNT(*) FROM {schema_name}.{table_name}\")).scalar()\n",
    "    print(f\"Total rows in {schema_name}.{table_name}: {count}\")\n",
    "\n",
    "    sample = conn.execute(text(f\"\"\"\n",
    "        SELECT * FROM {schema_name}.{table_name}\n",
    "        ORDER BY date ASC\n",
    "        LIMIT 5;\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "for row in sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05202",
   "metadata": {},
   "source": [
    "# Add Ticker Metadata for Yfinance OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51369eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 647\n",
      "Final row count: 648\n",
      "Row count increased by: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the 4 entries\n",
    "new_entries = pd.DataFrame([\n",
    "    {\"Ticker\": \"DOC\", \"StartDate\": \"2017-01-01\", \"EndDate\": \"2025-06-22\", \"SectorETF\": \"XLRE\"},\n",
    "])\n",
    "\n",
    "# Convert dates\n",
    "new_entries[\"StartDate\"] = pd.to_datetime(new_entries[\"StartDate\"]).dt.date\n",
    "new_entries[\"EndDate\"] = pd.to_datetime(new_entries[\"EndDate\"]).dt.date\n",
    "\n",
    "# Get row count before insert\n",
    "with engine.connect() as conn:\n",
    "    initial_count = conn.execute(text(\"SELECT COUNT(*) FROM quant.ticker_metadata\")).scalar()\n",
    "\n",
    "# Insert the 4 rows\n",
    "new_entries.to_sql(\n",
    "    name=\"ticker_metadata\",\n",
    "    con=engine,\n",
    "    schema=\"quant\",\n",
    "    if_exists=\"append\",\n",
    "    index=False,\n",
    "    method=\"multi\"\n",
    ")\n",
    "\n",
    "# Get row count after insert\n",
    "with engine.connect() as conn:\n",
    "    final_count = conn.execute(text(\"SELECT COUNT(*) FROM quant.ticker_metadata\")).scalar()\n",
    "    inserted_rows = conn.execute(text(\"\"\"\n",
    "        SELECT * FROM quant.ticker_metadata\n",
    "        WHERE \"Ticker\" IN ('DOC', 'LIN', 'TKO', 'VTRS')\n",
    "        ORDER BY \"Ticker\"\n",
    "    \"\"\")).fetchall()\n",
    "\n",
    "# Output\n",
    "print(f\"Initial row count: {initial_count}\")\n",
    "print(f\"Final row count: {final_count}\")\n",
    "print(f\"Row count increased by: {final_count - initial_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122a1a5",
   "metadata": {},
   "source": [
    "# Combine OHLCV Data Tables & Delete Old Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a193482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiingo rows: 1256197\n",
      "YFinance rows: 8512\n",
      "Combined rows: 1264709 (Expected: 1264709)\n",
      "Old tables dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create the new combined table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS quant.ticker_ohlcv (\n",
    "            Ticker TEXT,\n",
    "            Date DATE,\n",
    "            Close FLOAT,\n",
    "            High FLOAT,\n",
    "            Low FLOAT,\n",
    "            Open FLOAT,\n",
    "            Volume BIGINT,\n",
    "            AdjClose FLOAT,\n",
    "            AdjHigh FLOAT,\n",
    "            AdjLow FLOAT,\n",
    "            AdjOpen FLOAT,\n",
    "            AdjVolume BIGINT,\n",
    "            DivCash FLOAT,\n",
    "            SplitFactor FLOAT,\n",
    "            PRIMARY KEY (Ticker, Date)\n",
    "        );\n",
    "    \"\"\"))\n",
    "\n",
    "# Get original row counts\n",
    "with engine.connect() as conn:\n",
    "    count_tiingo = conn.execute(text(\"SELECT COUNT(*) FROM quant.ohlcv_tiingo\")).scalar()\n",
    "    count_yfinance = conn.execute(text(\"SELECT COUNT(*) FROM quant.ohlcv_yfinance\")).scalar()\n",
    "\n",
    "print(f\"Tiingo rows: {count_tiingo}\")\n",
    "print(f\"YFinance rows: {count_yfinance}\")\n",
    "\n",
    "# Insert data into new table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"INSERT INTO quant.ticker_ohlcv SELECT * FROM quant.ohlcv_tiingo\"))\n",
    "    conn.execute(text(\"INSERT INTO quant.ticker_ohlcv SELECT * FROM quant.ohlcv_yfinance\"))\n",
    "\n",
    "# Validate row count\n",
    "with engine.connect() as conn:\n",
    "    total_new = conn.execute(text(\"SELECT COUNT(*) FROM quant.ticker_ohlcv\")).scalar()\n",
    "\n",
    "print(f\"Combined rows: {total_new} (Expected: {count_tiingo + count_yfinance})\")\n",
    "\n",
    "# Drop old tables ONLY if row count is correct\n",
    "if total_new == count_tiingo + count_yfinance:\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(\"DROP TABLE quant.ohlcv_tiingo\"))\n",
    "        conn.execute(text(\"DROP TABLE quant.ohlcv_yfinance\"))\n",
    "    print(\"Old tables dropped successfully.\")\n",
    "else:\n",
    "    print(\"Row mismatch. Tables not dropped. Please check for duplicates or conflicts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad2f21",
   "metadata": {},
   "source": [
    "# Overview of Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ac49d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in 'quant' schema (4 total):\n",
      "\n",
      "Table: ticker_ohlcv\n",
      "Rows: 1264709\n",
      "Columns: 14\n",
      "     - ticker: TEXT\n",
      "     - date: DATE\n",
      "     - close: DOUBLE PRECISION\n",
      "     - high: DOUBLE PRECISION\n",
      "     - low: DOUBLE PRECISION\n",
      "     - open: DOUBLE PRECISION\n",
      "     - volume: BIGINT\n",
      "     - adjclose: DOUBLE PRECISION\n",
      "     - adjhigh: DOUBLE PRECISION\n",
      "     - adjlow: DOUBLE PRECISION\n",
      "     - adjopen: DOUBLE PRECISION\n",
      "     - adjvolume: BIGINT\n",
      "     - divcash: DOUBLE PRECISION\n",
      "     - splitfactor: DOUBLE PRECISION\n",
      "--------------------------------------------------\n",
      "Table: sector_etf_prices\n",
      "Rows: 23408\n",
      "Columns: 14\n",
      "     - etf: TEXT\n",
      "     - date: DATE\n",
      "     - close: DOUBLE PRECISION\n",
      "     - high: DOUBLE PRECISION\n",
      "     - low: DOUBLE PRECISION\n",
      "     - open: DOUBLE PRECISION\n",
      "     - volume: BIGINT\n",
      "     - adjclose: DOUBLE PRECISION\n",
      "     - adjhigh: DOUBLE PRECISION\n",
      "     - adjlow: DOUBLE PRECISION\n",
      "     - adjopen: DOUBLE PRECISION\n",
      "     - adjvolume: BIGINT\n",
      "     - divcash: DOUBLE PRECISION\n",
      "     - splitfactor: DOUBLE PRECISION\n",
      "--------------------------------------------------\n",
      "Table: macro_indicators\n",
      "Rows: 16658\n",
      "Columns: 7\n",
      "     - ticker: TEXT\n",
      "     - date: DATE\n",
      "     - close: DOUBLE PRECISION\n",
      "     - high: DOUBLE PRECISION\n",
      "     - low: DOUBLE PRECISION\n",
      "     - open: DOUBLE PRECISION\n",
      "     - volume: BIGINT\n",
      "--------------------------------------------------\n",
      "Table: ticker_metadata\n",
      "Rows: 648\n",
      "Columns: 4\n",
      "     - Ticker: TEXT\n",
      "     - StartDate: TEXT\n",
      "     - EndDate: TEXT\n",
      "     - SectorETF: TEXT\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inspector = inspect(engine)\n",
    "quant_tables = inspector.get_table_names(schema=\"quant\")\n",
    "\n",
    "print(f\"Tables in 'quant' schema ({len(quant_tables)} total):\\n\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for table in quant_tables:\n",
    "        # Get column info\n",
    "        columns = inspector.get_columns(table, schema=\"quant\")\n",
    "        col_info = [(col[\"name\"], str(col[\"type\"])) for col in columns]\n",
    "        num_cols = len(col_info)\n",
    "        \n",
    "        # Get row count\n",
    "        row_count = conn.execute(text(f\"SELECT COUNT(*) FROM quant.{table}\")).scalar()\n",
    "        \n",
    "        print(f\"Table: {table}\")\n",
    "        print(f\"Rows: {row_count}\")\n",
    "        print(f\"Columns: {num_cols}\")\n",
    "        for name, dtype in col_info:\n",
    "            print(f\"     - {name}: {dtype}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
